# -*- coding: utf-8 -*-
"""Data_Parser.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13UWTTox1I9e2oGMEiv3_tQ_rqFWGWa4e
"""

# from google.colab import drive
# drive.mount('/content/drive')

#https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data

import gzip
import json
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder

user_reviews_path = "/content/drive/My Drive/australian_user_reviews.json.gz"
user_items_path = "/content/drive/My Drive/australian_users_items.json.gz"
bundle_data_path = "/content/drive/My Drive/bundle_data.json.gz"
steam_games_path = "/content/drive/My Drive/steam_games.json.gz"
steam_reviews_path = "/content/drive/My Drive/steam_reviews.json.gz"

save_path = "/content/drive/My Drive/Results/"

size_user_reviews = 25799
size_user_items = 88311
size_bundle_data = 615
size_steam_games = 32135
size_steam_reviews = 7793069

def parse(path):
  g = gzip.open(path, 'r')
  for l in g:
    yield eval(l)

def output_parse_generator(path, max_entries):
	print('\nLoading data from ' + path + '\n')
	entries = []
	i = 0 
	for entry in parse(path):
		if i >= max_entries:
			break
		entries.append(entry)
		i = i+1
	return entries

#used by all recommenders
# Accepts a true/false array of size 5 to indicate what information is needed
#4:26 - 4:34 (8 minutes) for all data
def get_parsed_data(binarray):

	parsed_data = []

	if binarray[0]:
		parsed_data.append(output_parse_generator(user_reviews_path, size_user_reviews))
	if binarray[1]:
		parsed_data.append(output_parse_generator(user_items_path, size_user_items))
	if binarray[2]:
		parsed_data.append(output_parse_generator(bundle_data_path, size_bundle_data))
	if binarray[3]:
		parsed_data.append(output_parse_generator(steam_games_path, size_steam_games))
	if binarray[4]:
		parsed_data.append(output_parse_generator(steam_reviews_path, size_steam_reviews))

	return parsed_data

#used by Apriori Recommender and KNN User-Colaborative Recommender
def get_user_game_sparse_matrix():
	print('Parsing User_Items Data...')
	user_game_data = get_parsed_data([False, True, False, False, False])

	print('Retrieving Game Names...')
	only_games = []
	for user in user_game_data[0]:
		games = []
		for item in user['items']:
			games.append(item['item_name'])
		only_games.append(games)

	print('Encoding Transaction...')
	te = TransactionEncoder()
	sparse_matrix = te.fit(only_games).transform(only_games)

	num_users_with_games = 0
	for i in range(0, len(sparse_matrix)):
		
		for j in range(0, len(sparse_matrix[i])):
			if sparse_matrix[i][j]:
				num_users_with_games += 1
				break

	print('Users with games', 71504)# already calculated this once, no need to again
	print('Users', len(sparse_matrix))
	print('Games', len(sparse_matrix[0]))

	game_dataframe = pd.DataFrame(sparse_matrix, columns = te.columns_)

	print('Returned DataFrame of Games.')
	return game_dataframe

#takes in an index of which of the 5 files to look through
#takes in a key of the specific field to find all of the values for
#*NOTE* Does not currently work for keys in nested dictionaries
#returns a set of all of the possible values for the given key in the given file
def get_all_distinct_values(index, key):
	binarray = [False, False, False, False, False]
	binarray[index] = True

	values = set()
	data = get_parsed_data(binarray)# returns an array of one element (since only one index is updated in binarray)

	print('Finding all values under key \"'+key+'\"...')

	num_no_keys = 0
	for datum_dict in data[0]: # get dictionary at only element

		try:
			values.add(datum_dict[key])
		except TypeError:# occurs when encountering a list at datum_dict[key]
			values.update(datum_dict[key])
		except KeyError:# entry does not have a value for the key
			num_no_keys += 1

	print(str(num_no_keys)+' entries did not have values for the specified key \"'+key+'\"')

	return values